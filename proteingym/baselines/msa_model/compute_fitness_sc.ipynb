{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from scipy.stats import spearmanr\n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "from msa_model.modules import MSAContactModel\n",
    "from msa_model.dataset import MSA, prepare_additional_molecule_feats, aa2tok_d, prepare_msa_masks\n",
    "from msa_model.utils import fit_seq_weight_mixture_model, evaluate_contact_prediction\n",
    "\n",
    "from compute_fitness_utils import *\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "print(f\"Using device: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "sys.path.append(\"../utils/\")\n",
    "from scoring_utils import get_optimal_window, set_mutant_offset, undo_mutant_offset\n",
    "from data_utils import DMS_file_cleanup\n",
    "from msa_utils import MSA_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute scores for DMS: OTU7A_HUMAN_Tsuboyama_2023_2L2D\n",
      "df shape: (635, 4)\n",
      "Number of DMS positions: 42\n",
      "Layer 0 lambda init: 0.2001953125\n",
      "Layer 1 lambda init: 0.35546875\n",
      "Layer 2 lambda init: 0.470703125\n",
      "Layer 3 lambda init: 0.5546875\n",
      "Layer 4 lambda init: 0.62109375\n",
      "Layer 5 lambda init: 0.66796875\n",
      "Layer 6 lambda init: 0.69921875\n",
      "Layer 7 lambda init: 0.7265625\n",
      "Layer 8 lambda init: 0.74609375\n",
      "Layer 9 lambda init: 0.7578125\n",
      "Layer 10 lambda init: 0.76953125\n",
      "Layer 11 lambda init: 0.77734375\n",
      "Layer 12 lambda init: 0.78515625\n",
      "Layer 13 lambda init: 0.7890625\n",
      "Layer 14 lambda init: 0.7890625\n",
      "Layer 15 lambda init: 0.79296875\n",
      "Layer 16 lambda init: 0.796875\n",
      "Layer 17 lambda init: 0.796875\n",
      "Layer 18 lambda init: 0.796875\n",
      "Layer 19 lambda init: 0.796875\n",
      "Layer 20 lambda init: 0.796875\n",
      "Layer 21 lambda init: 0.80078125\n",
      "Missing keys:  ['0.weight' '1.weight']\n",
      "Unexpected keys:  []\n",
      "Computing model scores...\n",
      "Number of sequences in MSA (before preprocessing): 1359071\n",
      "Calculating proportion of gaps\n",
      "Proportion of sequences dropped due to fraction of gaps: 0.0%\n",
      "Proportion of non-focus columns removed: 0.0%\n",
      "Proportion of sequences dropped due to indeterminate AAs: 0.1%\n",
      "Number of sequences after preprocessing: 1357721\n",
      "Data Shape = (1357721, 42, 20)\n",
      "Computing sequence weights\n",
      "Set number of threads to: 16\n"
     ]
    }
   ],
   "source": [
    "# Parse command line arguments\n",
    "class args_obj():\n",
    "    def __init__(self):\n",
    "        self.model_path = \"/home/gridsan/yakiyama/AF_inv_covariance/model_checkpoints/train_presoftmax_diff_attn_04102025_fullft_no_adversarial/model_step_18000.pt\"\n",
    "        self.contact_model_path = \"/home/gridsan/yakiyama/AF_inv_covariance/model_checkpoints/train_contact_presoftmax_diff_attn_04142025/model_final.pt\"\n",
    "        self.sequence = None\n",
    "        self.dms_input = \"/home/gridsan/yakiyama/AF_inv_covariance/misc_data/proteingym/DMS_ProteinGym_substitutions/\" # A0A140D2T1_ZIKV_Sourisseau_2019.csv\"\n",
    "        self.dms_index = 113\n",
    "        self.dms_mapping = \"/home/gridsan/yakiyama/ProteinGym/reference_files/DMS_substitutions.csv\"\n",
    "        self.mutation_col = \"mutant\"\n",
    "        self.dms_output = \"test.csv\"\n",
    "        self.offset_idx = 1\n",
    "        self.scoring_strategy = \"masked-marginals\"\n",
    "        self.msa_path = \"/home/gridsan/yakiyama/AF_inv_covariance/misc_data/proteingym/DMS_msa_files/\"\n",
    "        self.msa_sampling_strategy = \"sequence-reweighting\"\n",
    "        self.msa_samples = 400\n",
    "        self.msa_weights_folder = \"msa_weights/\" #\"/home/ubuntu/AF_inv_covariance/misc_data/proteingym/DMS_msa_weights/\"\n",
    "        self.filter_msa = False\n",
    "        self.scoring_window = \"optimal\"\n",
    "        self.overwrite_prior_scores = True\n",
    "        self.hhfilter_min_cov = 75\n",
    "        self.hhfilter_max_seq_id = 90\n",
    "        self.hhfilter_min_seq_id = 0\n",
    "        self.path_to_hhfilter = \"/home/gridsan/yakiyama/other_tools/hhfilter/bin/hhfilter\"\n",
    "        self.seeds = [0]\n",
    "        self.num_cpus = 16\n",
    "\n",
    "args = args_obj()\n",
    "\n",
    "# Load deep mutational scan\n",
    "mutant_col = args.mutation_col  # Default \"mutant\"\n",
    "# If index of DMS file is provided\n",
    "if args.dms_index is not None:\n",
    "    mapping_protein_seq_DMS = pd.read_csv(args.dms_mapping)\n",
    "    DMS_id = mapping_protein_seq_DMS[\"DMS_id\"][args.dms_index]\n",
    "    print(\"Compute scores for DMS: \"+str(DMS_id))\n",
    "    row = mapping_protein_seq_DMS[mapping_protein_seq_DMS[\"DMS_id\"]==DMS_id]\n",
    "    if len(row) == 0:\n",
    "        raise ValueError(\"No mappings found for DMS: \"+str(DMS_id))\n",
    "    elif len(row) > 1:\n",
    "        raise ValueError(\"Multiple mappings found for DMS: \"+str(DMS_id))\n",
    "    \n",
    "    row = row.iloc[0]\n",
    "    row = row.replace(np.nan, \"\")  # Makes it more manageable to use in strings\n",
    "\n",
    "    args.sequence = row[\"target_seq\"].upper()\n",
    "    args.dms_input = str(args.dms_input)+os.sep+row[\"DMS_filename\"]\n",
    "\n",
    "    mutant_col = row[\"DMS_mutant_column\"] if \"DMS_mutant_column\" in mapping_protein_seq_DMS.columns else mutant_col\n",
    "    args.dms_output=str(args.dms_output)+os.sep+DMS_id+'.csv'\n",
    "    \n",
    "    target_seq_start_index = row[\"start_idx\"] if \"start_idx\" in mapping_protein_seq_DMS.columns and row[\"start_idx\"]!=\"\" else 1\n",
    "    target_seq_end_index = target_seq_start_index + len(args.sequence) \n",
    "\n",
    "    # Get MSA file paths and indices\n",
    "    msa_filename = row[\"MSA_filename\"]\n",
    "    if msa_filename == \"\":\n",
    "        raise ValueError(\"No MSA found for DMS: \"+str(DMS_id))\n",
    "\n",
    "    args.msa_path= str(args.msa_path)+os.sep+msa_filename # msa_path is expected to be the path to the directory where MSAs are located.\n",
    "        \n",
    "    msa_start_index = int(row[\"MSA_start\"]) if \"MSA_start\" in mapping_protein_seq_DMS.columns else 1\n",
    "    msa_end_index = int(row[\"MSA_end\"]) if \"MSA_end\" in mapping_protein_seq_DMS.columns else len(args.sequence)\n",
    "    \n",
    "    MSA_weight_file_name = args.msa_weights_folder + os.sep + row[\"weight_file_name\"] if (\"weight_file_name\" in mapping_protein_seq_DMS.columns and args.msa_weights_folder is not None) else None\n",
    "    if ((target_seq_start_index!=msa_start_index) or (target_seq_end_index!=msa_end_index)):\n",
    "        args.sequence = args.sequence[msa_start_index-1:msa_end_index]\n",
    "        target_seq_start_index = msa_start_index\n",
    "        target_seq_end_index = msa_end_index\n",
    "    df = pd.read_csv(args.dms_input)\n",
    "# If no index of DMS file is provided\n",
    "else:\n",
    "    DMS_id = str(args.dms_input).split(os.sep)[-1].split('.csv')[0]\n",
    "    args.dms_output=str(args.dms_output)+os.sep+DMS_id+'.csv'\n",
    "    target_seq_start_index = args.offset_idx\n",
    "    args.sequence = args.target_seq.upper()\n",
    "    if (args.MSA_start is None) or (args.MSA_end is None): \n",
    "        if args.msa_path: print(\"MSA start and end not provided -- Assuming the MSA is covering the full WT sequence\")\n",
    "        args.MSA_start = 1\n",
    "        args.MSA_end = len(args.target_seq)\n",
    "    msa_start_index = args.MSA_start\n",
    "    msa_end_index = args.MSA_end\n",
    "    MSA_weight_file_name = args.msa_weights_folder + os.sep + args.weight_file_name if args.msa_weights_folder is not None else None\n",
    "    df = pd.read_csv(args.dms_input)\n",
    "# Check if the dataframe is empty\n",
    "if len(df) == 0:\n",
    "    raise ValueError(\"No rows found in the dataframe\")\n",
    "print(f\"df shape: {df.shape}\", flush=True)\n",
    "# Get all variant positions\n",
    "dms_positions = set(df[mutant_col].map(lambda x: re.findall(r'[A-Z](\\d+)[A-Z]', x)).explode().astype(int).tolist())\n",
    "print(f\"Number of DMS positions: {len(dms_positions)}\", flush=True)\n",
    "\n",
    "# Load MSA model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_kwargs = dict(\n",
    "    dim_msa_input = 28,\n",
    "    dim_pairwise = 256,\n",
    "    dim_msa = 464,\n",
    "    dim_logits = 26,\n",
    "    msa_module_kwargs = dict(\n",
    "        depth = 22,\n",
    "        opm_kwargs = dict(\n",
    "            dim_opm_hidden = 16,\n",
    "            outer_product_flavor = \"presoftmax_differential_attention\",\n",
    "            seq_attn = True,\n",
    "            dim_qk = 128,\n",
    "            chunk_size = None,\n",
    "            return_seq_weights = True,\n",
    "            return_attn_logits = False,\n",
    "            lambda_init = None,\n",
    "            eps = 1e-32,\n",
    "        ),\n",
    "        pwa_kwargs = dict(\n",
    "            heads = 8,\n",
    "            dim_head = 32,\n",
    "            dropout = 0.1,\n",
    "            dropout_type = \"row\",\n",
    "        ),\n",
    "        pairwise_block_kwargs = dict(\n",
    "            tri_mult_dim_hidden = None,\n",
    "            use_triangle_attn = False,\n",
    "            use_triangle_updates = True\n",
    "        )\n",
    "    ),\n",
    "    relative_position_encoding_kwargs = dict(\n",
    "        r_max = 32,\n",
    "        s_max = 2,\n",
    "    ),\n",
    "    return_msa_repr = False,\n",
    "    return_pairwise_repr = True,\n",
    "    query_only = True\n",
    ")\n",
    "checkpoint_path = args.model_path\n",
    "model = MSAContactModel(pretrained_weights_path=checkpoint_path, pretrained_is_final=False, msa_model_kwargs=model_kwargs, compiled_checkpoint=True, dim_contact_hidden=128)\n",
    "\n",
    "# Load contact model weights\n",
    "contact_checkpoint_path = args.contact_model_path\n",
    "contact_checkpoint = torch.load(contact_checkpoint_path, weights_only=True)\n",
    "contact_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in contact_checkpoint.items() if \"contact\" in k}\n",
    "_, _ = model.load_state_dict(contact_state_dict, strict=False)\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "\n",
    "# Compute fitness scores\n",
    "print(\"Computing model scores...\")\n",
    "args.offset_idx = msa_start_index\n",
    "# Process MSA\n",
    "processed_msa = process_msa(filename=args.msa_path, weight_filename=MSA_weight_file_name, filter_msa=args.filter_msa, hhfilter_min_cov=args.hhfilter_min_cov, hhfilter_max_seq_id=args.hhfilter_max_seq_id, hhfilter_min_seq_id=args.hhfilter_min_seq_id, path_to_hhfilter=args.path_to_hhfilter, num_cpus=args.num_cpus)\n",
    "if os.path.exists(args.dms_output):\n",
    "    prior_score_df = pd.read_csv(args.dms_output)\n",
    "    if not args.overwrite_prior_scores:\n",
    "        print(\"Prior scores found -- skipping model scoring\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"Overwriting prior scores\")\n",
    "else:\n",
    "    prior_score_df = None\n",
    "seed = args.seeds[0]\n",
    "data = [sample_msa(sampling_strategy=args.msa_sampling_strategy, filename=args.msa_path, nseq=args.msa_samples, weight_filename=MSA_weight_file_name, processed_msa=processed_msa, random_seed=seed)]\n",
    "assert (args.scoring_strategy in [\"masked-marginals\",\"pseudo-ppl\"]), \"Zero-shot scoring strategy not supported with MSA Transformer\"\n",
    "\n",
    "# Prepare data for MSA model\n",
    "nTokenTypes = len(np.unique(list(aa2tok_d.values())))\n",
    "msa_onehot_t, tokenized_msa_t, mask, msa_mask, full_mask, pairwise_mask, additional_feats_t = prepare_msa_inputs(data)\n",
    "mask, msa_mask, full_mask, pairwise_mask, additional_feats_t = mask.to(device), msa_mask.to(device), full_mask.to(device), pairwise_mask.to(device), additional_feats_t.to(device)\n",
    "\n",
    "# Compute masked-marginals scores\n",
    "if args.scoring_strategy == \"masked-marginals\":\n",
    "    all_token_probs = []\n",
    "    for i in tqdm(range(tokenized_msa_t.size(1)), desc=\"Scoring masked-marginals\"):\n",
    "        # Skip if position is not in DMS\n",
    "        if i+args.offset_idx not in dms_positions:\n",
    "            zero_t = torch.zeros((1, 26))\n",
    "            all_token_probs.append(zero_t)\n",
    "            continue\n",
    "        tokenized_msa_masked_t = tokenized_msa_t.clone()\n",
    "        tokenized_msa_masked_t[0, i] = aa2tok_d['MASK']  # mask out first sequence\n",
    "        if tokenized_msa_t.size(-1) > 1024:\n",
    "            large_tokenized_msa_masked_t=tokenized_msa_masked_t.clone()\n",
    "            start, end = get_optimal_window(mutation_position_relative=i, seq_len_wo_special=len(args.sequence)+2, model_window=1024)\n",
    "            print(\"Start index {} - end index {}\".format(start,end))\n",
    "            tokenized_msa_masked_t = large_tokenized_msa_masked_t[:,:,start:end]\n",
    "        else:\n",
    "            start=0\n",
    "        with torch.no_grad():\n",
    "            msa_masked_onehot_t = one_hot(tokenized_msa_masked_t, num_classes=nTokenTypes).float().unsqueeze(0).to(device)\n",
    "            token_probs = torch.log_softmax(\n",
    "                model(\n",
    "                    additional_molecule_feats = additional_feats_t,\n",
    "                    msa = msa_masked_onehot_t,\n",
    "                    mask = mask,\n",
    "                    msa_mask = msa_mask,\n",
    "                    pairwise_mask = pairwise_mask,\n",
    "                    full_mask = full_mask\n",
    "                )['logits'], dim=-1\n",
    "            )\n",
    "        all_token_probs.append(token_probs[:, 0, i-start].detach().cpu())  # vocab size\n",
    "    token_probs = torch.cat(all_token_probs, dim=0).unsqueeze(0)\n",
    "    df[f\"MSA_model\"] = df.apply(\n",
    "        lambda row: label_row(\n",
    "            row[mutant_col], args.sequence, token_probs.detach().cpu(), aa2tok_d, args.offset_idx\n",
    "        ),\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['DMS_score'], df['MSA_model'])\n",
    "spearmanr(df['DMS_score'], df['MSA_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_l = glob(\"results/*\")\n",
    "files_l = sorted(files_l)\n",
    "for file in files_l:\n",
    "    df = pd.read_csv(file)\n",
    "    rho, _ = spearmanr(df['MSA_model'], df['DMS_score'])\n",
    "    print(f\"{file}\\n\\t{rho}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
